.prod-test:
  extends:
    - .ubuntu-nix-docker
    - .rules-prod-tests
  stage: prod-tests
  dependencies: []  # don't copy artifacts from other jobs by default
  variables:
    SHELL_WRAPPER: "/usr/bin/time"
  artifacts:
    when: always
    paths:
      - $CI_JOB_STAGE/$CI_JOB_NAME
  tags:
    - ic-prod-tests
    - docker
    - ubuntu

.prod-stress-test:
  extends: .prod-test
  resource_group: $TESTNET
  variables:
    cd_target_env: "STRESS"

.prod-hourly-test:
  extends: .prod-test
  resource_group: $TESTNET
  variables:
    cd_target_env: "HOURLY"

.prod-nightly-test:
  extends: .prod-test
  # [IDX-2138] Experiment to run all stages in parallel. Multiple testnets are provided.
  # A testnet will still only run one test at a time.
  needs: []
  variables:
    cd_target_env: "NIGHTLY"
  timeout: 3 hours

# The prod nightly test groups partition the prod tests to parallelize across
# multiple testnets. In our case, we have 5 testnets so create 5 prod groups.
# This is a short term solution to speed up nightly tests, until testnets can be
# deployed dynamically using farm.

# The first testing group is reserved for the longest running test [6 hours].
# For new tests, try to distribute them evenly across testing groups. You can also
# inspect the Honeycomb traces for the rc--* branches to determine which group is
# the least utilized.
#
# Find traces here: https://ui.honeycomb.io/dfinity/datasets/gitlab-ci-dfinity/result/G8UcQLcnfd3/a/3wQM1irQAYr/GitLab-Release-Candidate-Pipeline-Status?tab=traces
# The overall dashboard is available here: https://ui.honeycomb.io/dfinity/board/58LbKzZgjTA/GitLab-Release-Candidate-Performance
.prod-nightly-test-group-1-reserved:
  stage: prod-tests-01
  extends: .prod-nightly-test
  variables:
    TESTNET: $TESTNET1
  resource_group: $TESTNET1

.prod-nightly-test-group-2:
  stage: prod-tests-02
  extends: .prod-nightly-test
  variables:
    TESTNET: $TESTNET2
    cd_debug_target: "true"
  resource_group: $TESTNET2

.prod-nightly-test-group-3:
  stage: prod-tests-03
  extends: .prod-nightly-test
  variables:
    TESTNET: $TESTNET3
    cd_debug_target: "true"
  resource_group: $TESTNET3

.prod-nightly-test-group-4:
  stage: prod-tests-04
  extends: .prod-nightly-test
  variables:
    TESTNET: $TESTNET4
  resource_group: $TESTNET4

.prod-nightly-test-group-5:
  stage: prod-tests-05
  extends: .prod-nightly-test
  variables:
    TESTNET: $TESTNET5
  resource_group: $TESTNET5

.prod-slo-test:
  extends: .prod-test
  resource_group: $TESTNET
  variables:
    cd_target_env: "SLO"
  timeout: 3 hours

# TESTING NOTE:
# $SHELL_WRAPPER allows us to emulate CI runs without actually executing the complicated and time-consuming
#      operations.
#      While validating the CI configuration, "$SHELL_WRAPPER" will be substituted with "echo"
#      During actual execution, "$SHELL_WRAPPER" will be substituted with "time", i.e. will time the execution

# NIGHTLY TESTS

# rejoin_test: 15min runtime, 8 canisters installed
# Kill one node for the majority of time during the 15min runtime, restart it and check if it contributes to IC later
rejoin-nightly:
  extends: .prod-nightly-test-group-2
  script:
    - |
      set -eExou pipefail
      if [[ -z "$TESTNET2" ]]; then echo "Please set the TESTNET2 environment variable for proper resource_group locking"; exit 1; fi
      git fetch
      GIT_REVISION=$("$CI_PROJECT_DIR"/gitlab-ci/src/artifacts/newest_sha_with_disk_image.sh "$DISKIMG_BRANCH")
      export GIT_REVISION

      $SHELL_WRAPPER timeout 46m ./testnet/tests/scripts/rejoin_test.sh "$TESTNET" 900 8 8 normal "$CI_JOB_STAGE/$CI_JOB_NAME"

# SCENARIO TEST deploy_nns_prod_state: Tests the NNS state deployment script
nns-state-deployment-test-nightly:
  extends: .prod-nightly-test-group-2
  allow_failure: true #nns state deployment is not needed for release of replica
  script:
    - |
      set -eExou pipefail
      if [[ -z "$TESTNET2" ]]; then echo "Please set the TESTNET2 environment variable for proper resource_group locking"; exit 1; fi
      git fetch
      GIT_REVISION=$("$CI_PROJECT_DIR"/gitlab-ci/src/artifacts/newest_sha_with_disk_image.sh "$DISKIMG_BRANCH")
      export GIT_REVISION
      $SHELL_WRAPPER timeout 1h ./testnet/tests/scripts/nns_state_deployment_test.sh "$TESTNET" "$CI_JOB_STAGE/$CI_JOB_NAME"

# SCENARIO TEST rejoin: 15min runtime, 8 canisters installed
# Kill one node for the majority of time during the 15min runtime, restart it and check if it contributes to IC later
rejoin-test-slo:
  extends: .prod-slo-test
  script:
    - |
      set -eExou pipefail
      git fetch
      GIT_REVISION=$("$CI_PROJECT_DIR"/gitlab-ci/src/artifacts/newest_sha_with_disk_image.sh "$DISKIMG_BRANCH")
      export GIT_REVISION

      $SHELL_WRAPPER ./testnet/tests/scripts/rejoin_test.sh "$TESTNET" 900 8 8 large "$CI_JOB_STAGE/$CI_JOB_NAME"

# 56-nns-tests-slo: run some tests on topology with 56 nodes in the NNS subents
56-nns-tests-slo:
  extends: .prod-slo-test
  allow_failure: true #56 node NNS test is not needed for release of replica
  script:
    - |
      set -eExou pipefail
      git fetch
      GIT_REVISION=$("$CI_PROJECT_DIR"/gitlab-ci/src/artifacts/newest_sha_with_disk_image.sh "$DISKIMG_BRANCH")
      export GIT_REVISION

      $SHELL_WRAPPER timeout 3h ./testnet/tests/scripts/subnet_update_workload.sh "$TESTNET" 7200 280 1k 56_nns replica_nodes "$CI_JOB_STAGE/$CI_JOB_NAME"
  timeout: 4 hours

# 46-nns-tests-slo: run some tests on topology with 46 nodes in the NNS subents
46-nns-tests-slo:
  extends: .prod-slo-test
  allow_failure: true #46 nodes in NNS test is not needed for release of replica
  script:
    - |
      set -eExou pipefail
      git fetch
      GIT_REVISION=$("$CI_PROJECT_DIR"/gitlab-ci/src/artifacts/newest_sha_with_disk_image.sh "$DISKIMG_BRANCH")
      export GIT_REVISION

      $SHELL_WRAPPER timeout 160m ./testnet/tests/scripts/subnet_update_workload.sh "$TESTNET" 7200 200 1k 46_nns replica_nodes "$CI_JOB_STAGE/$CI_JOB_NAME"
  timeout: 4 hours

icos-continuous-upgrade-nightly:
  extends: .prod-nightly-test
  variables:
    cd_target_env: "IC_UPGRADE"
  needs: []  # allow starting immediately
  dependencies: []  # don't copy artifacts from other jobs
  timeout: 6 hours
  artifacts:
    when: always
    paths:
      - ic-os/guestos/test-out/e2e-continuous-upgrade
  script:
    - |
      set -eExou pipefail
      GIT_REVISION=$("$CI_PROJECT_DIR"/gitlab-ci/src/artifacts/newest_sha_with_disk_image.sh origin/master)
      export GIT_REVISION

      ls -lah /dev/kvm /dev/net/tun

      mkdir -p gitlab-runner-tmp; cd gitlab-runner-tmp

      "${CI_PROJECT_DIR}"/gitlab-ci/src/artifacts/rclone_download.py --git-rev "$GIT_REVISION" --remote-path=guest-os --out=guest-os --latest-to
      "${CI_PROJECT_DIR}"/gitlab-ci/src/artifacts/rclone_download.py --git-rev "$GIT_REVISION" --remote-path=canisters --out=artifacts --latest-to
      "${CI_PROJECT_DIR}"/gitlab-ci/src/artifacts/rclone_download.py --git-rev "$GIT_REVISION" --remote-path=release --out=artifacts --latest-to

      ls -R artifacts guest-os
      (cd artifacts; for f in *.gz; do gunzip "$f"; done; chmod u+x ./*)
      (cd guest-os/disk-img; for f in *.gz; do gunzip "$f"; done)
      (cd guest-os/disk-img; for f in *.tar; do tar -xf "$f"; done)
      ls -R artifacts guest-os

      # Prepare network. There are more convenient ways to do it if requisite
      # services are set up (which they aren't in a simple docker runner),
      # but probably also helpful for debugging to have this "explicit" for now.

      sudo ip tuntap add ipv6_ic_node0 mode tap
      sudo ip link set dev ipv6_ic_node0 up

      sudo ip tuntap add ipv6_ic_node1 mode tap
      sudo ip link set dev ipv6_ic_node1 up

      sudo ip link add name ipv6_ic type bridge
      sudo ip link set ipv6_ic_node0 master ipv6_ic
      sudo ip link set ipv6_ic_node1 master ipv6_ic
      sudo ip link set dev ipv6_ic up

      sudo ip addr add fd00:2:1:1:1::1/64 dev ipv6_ic

      # Output what image we are using:
      GUESTOS_IMG="$(pwd)/guest-os/disk-img/disk.img"
      UPGRADE_IMG="$(pwd)/guest-os/update-img/update-img.tar.gz"
      VERSION=$(cat "$(pwd)/guest-os/disk-img/version.txt")
      echo "Initial GuestOS image: ${GUESTOS_IMG} at version ${VERSION}"
      echo "Upgrade GuestOS image: ${UPGRADE_IMG}"

      mkdir -p "${CI_PROJECT_DIR}/ic-os/guestos/test-out/e2e-continuous-upgrade"
      # Actual test script, sets up VMs and drives the test.
      $SHELL_WRAPPER "${CI_PROJECT_DIR}/ic-os/guestos/tests/e2e-continuous-upgrade-testing.py" \
          --vmtoolscfg=internal \
          --disk_image "${GUESTOS_IMG}" \
          --ic_prep_bin "$(pwd)/artifacts/ic-prep" \
          --install_nns_bin "$(pwd)/artifacts/ic-nns-init" \
          --script_dir "${CI_PROJECT_DIR}/ic-os/guestos/tests/scripts/" \
          --upgrade_tar "${UPGRADE_IMG}" \
          --ic_admin_bin "$(pwd)/artifacts/ic-admin" \
          --nns_canisters "$(pwd)/artifacts/" \
          --log_directory "${CI_PROJECT_DIR}/ic-os/guestos/test-out/e2e-continuous-upgrade" \
          --version "$GIT_REVISION" \
          --is_upgrade_test

.run-farm-based-test: &run-farm-based-test |
  WORKING_DIR="${CI_PROJECT_DIR}/working_dir/"
  mkdir -p "$WORKING_DIR"
  IC_VERSION_ID=$("$CI_PROJECT_DIR"/gitlab-ci/src/artifacts/newest_sha_with_disk_image.sh "$DISKIMG_BRANCH")
  export IC_VERSION_ID
  if [[ -z "${REPLICA_LOG_DEBUG_OVERRIDES:-}" ]]; then
    REPLICA_LOG_DEBUG_OVERRIDES_OPTS=()
  else
    REPLICA_LOG_DEBUG_OVERRIDES_OPTS=("--replica-log-debug-overrides=${REPLICA_LOG_DEBUG_OVERRIDES}")
  fi

  # Logs produced during execution of the system tests are stored away in a structured way, i.e. one file per test.
  # Unstructured logs, i.e. those which bypassed the logger and were outputed directly to stdout/stderr,
  # are forwarded to a separate file.
  # All lines produced by the logger share the same structure: e.g. "Nov 10 13:20:30.931 INFO ...".
  # Hence, the fourth column can be used to distinguish structured from unstructured logs.
  $SHELL_WRAPPER "${CI_PROJECT_DIR}/rs/tests/run-system-tests.py" \
    --suite="${SUITE_NAME}" \
    --working-dir="${WORKING_DIR}" \
    "${REPLICA_LOG_DEBUG_OVERRIDES_OPTS[@]}" \
  | tee "${WORKING_DIR}/unstructured-logs.log" \
  | awk '$4 ~ /CRIT|ERRO|WARN|INFO|DEBG|TRCE/'

.system-tests:
  extends: .prod-test
  needs: []  # allow starting immediately
  artifacts:
    when: always
    paths:
      - "$CI_PROJECT_DIR/working_dir/"
  allow_failure: false

.with-monitoring:
  variables:
    # we need debug-level logs from the following components for IC policy monitoring
    REPLICA_LOG_DEBUG_OVERRIDES: "ic_consensus::consensus::batch_delivery,ic_artifact_manager::processors,ic_consensus::consensus,ic_transport::control_plane"

system-tests-hourly:
  extends:
    - .system-tests
    - .with-monitoring
  variables:
    SUITE_NAME: "hourly"
    cd_target_env: "HOURLY"
  script:
    - *run-farm-based-test

rosetta-hourly:
  extends:
    - .system-tests
    - .with-monitoring
  variables:
    SUITE_NAME: "rosetta"
    cd_target_env: "HOURLY"
  script:
    - |
      # Ensure that rosetta 3rd-party dependencies are available
      # shellcheck disable=SC1090
      . "${CI_PROJECT_DIR}/rs/tests/prepare-rosetta-deps.sh"
    - *run-farm-based-test

.system-tests-wasm-generator:
  extends: .system-tests
  variables:
    SUITE_NAME: "wasm_generator"
  script:
    - |
      export RANDOM_CANISTERS_BASE_DIR="${CI_PROJECT_DIR}/test_canisters"
      mkdir "${RANDOM_CANISTERS_BASE_DIR}"
      export TEST_MODULES="${CI_PROJECT_DIR}/testnet/tests/scripts/test_modules"
      $SHELL_WRAPPER "${TEST_MODULES}/wasm-generator/wasm-generator.sh" 20 "${NUMBER_OF_CANISTERS}"
    - *run-farm-based-test

wasm-generator-hourly:
  extends:
    - .system-tests-wasm-generator
    - .with-monitoring
  variables:
    cd_target_env: "HOURLY"
    NUMBER_OF_CANISTERS: 100

wasm-generator-nightly:
  extends:
    - .system-tests-wasm-generator
    - .with-monitoring
  stage: prod-tests-04
  variables:
    NUMBER_OF_CANISTERS: 1000
    cd_target_env: "NIGHTLY"
    SYSTEM_TESTS_TIMEOUT: "7200"  # 2 h
  timeout: 3 hours

wasm-generator-slo:
  extends: .system-tests-wasm-generator
  variables:
    NUMBER_OF_CANISTERS: 2000
    SYSTEM_TESTS_TIMEOUT: "7200"  # 2 h
    cd_target_env: "SLO"
  timeout: 3 hours

system-tests-short-nightly:
  extends:
    - .system-tests
    - .with-monitoring
  variables:
    SUITE_NAME: "nightly_short_duration"
    cd_target_env: "NIGHTLY"
    SYSTEM_TESTS_TIMEOUT: "3000"  # 50 mins
  timeout: 60 minutes # this CI job timeout should be a bit larger than internal SYSTEM_TESTS_TIMEOUT
  script:
    - *run-farm-based-test

system-tests-default-subnet-query-workload-long-duration-nightly:
  extends:
    - .system-tests
  variables:
    SUITE_NAME: "nightly_default_subnet_query_workload_long_duration_test"
    cd_target_env: "NIGHTLY"
    SYSTEM_TESTS_TIMEOUT: "30000" # 8h 20min
  timeout: 580 minutes # this CI job timeout should be a bit larger than internal SYSTEM_TESTS_TIMEOUT
  script:
    - echo "ulimit -n = $(ulimit -n)"
    - *run-farm-based-test

system-tests-default-subnet-update-workload-long-duration-nightly:
  extends:
    - .system-tests
  variables:
    SUITE_NAME: "nightly_default_subnet_update_workload_long_duration_test"
    cd_target_env: "NIGHTLY"
    SYSTEM_TESTS_TIMEOUT: "30000" # 8h 20min
  timeout: 580 minutes # this CI job timeout should be a bit larger than internal SYSTEM_TESTS_TIMEOUT
  script:
    - echo "ulimit -n = $(ulimit -n)"
    - *run-farm-based-test

system-tests-default-subnet-update-workload-large-payload-nightly:
  extends:
    - .system-tests
  variables:
    SUITE_NAME: "nightly_default_subnet_update_workload_large_payload"
    cd_target_env: "NIGHTLY"
    SYSTEM_TESTS_TIMEOUT: "30000" # 8h 20min
  timeout: 580 minutes # this CI job timeout should be a bit larger than internal SYSTEM_TESTS_TIMEOUT
  script:
    - echo "ulimit -n = $(ulimit -n)"
    - *run-farm-based-test

# Stress tests: Run given job sequence every N minutes so we can easily collect failure data
icos-deploy-stress:
  extends: .prod-stress-test
  parallel:
    matrix:
      - DEPLOY_FLAVOR: single-dc
        TESTNET:
        - cdhourlydebug01
        - cdhourlydebug02
      - DEPLOY_FLAVOR: multi-dc
        TESTNET:
        - cdhourlydebug03
        - cdhourlydebug04
  script:
    - |
      set -eExou pipefail
      git fetch
      GIT_REVISION=$("$CI_PROJECT_DIR"/gitlab-ci/src/artifacts/newest_sha_with_disk_image.sh "$DISKIMG_BRANCH")
      export GIT_REVISION

      $SHELL_WRAPPER timeout 10m ./testnet/tools/icos_deploy.sh $TESTNET --git-revision "$GIT_REVISION"

.prod-hotfix-test:
  extends:
    - .prod-test
    - .rules-prod-hotfix-tests
  variables:
    cd_target_env: "HOTFIX"
  timeout: 55 minutes

rejoin-hotfix:
  extends: .prod-hotfix-test
  variables:
    TESTNET: $TESTNET2
  resource_group: $TESTNET2
  script:
    - |
      set -eExou pipefail
      git fetch
      GIT_REVISION=$("$CI_PROJECT_DIR"/gitlab-ci/src/artifacts/newest_sha_with_disk_image.sh "$DISKIMG_BRANCH")
      export GIT_REVISION

      $SHELL_WRAPPER ./testnet/tests/scripts/rejoin_test.sh "$TESTNET" 900 8 8 normal "$CI_JOB_STAGE/$CI_JOB_NAME"

system-tests-hotfix:
  extends:
    - .system-tests
    - .with-monitoring
    - .prod-hotfix-test
  variables:
    SUITE_NAME: "hotfix"
  script:
    - *run-farm-based-test

system-tests-staging-nightly:
  extends:
    - .system-tests
  variables:
    SUITE_NAME: "staging"
    SYSTEM_TESTS_TIMEOUT: "30000" #8h 20min
    cd_target_env: "NIGHTLY"
  timeout: 580 minutes # this CI job timeout should be a bit larger than internal SYSTEM_TESTS_TIMEOUT
  script:
    - *run-farm-based-test
  allow_failure: true

ingress-manager-proptests-nightly:
  extends:
    - .prod-test
  stage: prod-tests
  variables:
    cd_target_env: "NIGHTLY"
  timeout: 70 minutes
  script:
    - |
      cd "${CI_PROJECT_DIR}/rs"
      $SHELL_WRAPPER cargo test --release -p ic-ingress-manager --features proptest proptest
