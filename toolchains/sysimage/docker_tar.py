#!/usr/bin/env python3
#
# Runs a docker build and extract the built container as a single flattened
# tar file. All timestamps and other non-reproducible effects of docker build
# (e.g. order of files) are squashed in order to build a reproducible tarball.
#
# The script is expected to be called with the arguments to be given to "docker build".
# The resulting tar file will be written to stdout.
#
# Call example:
#   docker_tar dockerdir --build-arg foo=bar > tree.tar
#
import argparse
import atexit
import hashlib
import io
import json
import os
import re
import subprocess
import sys
import tarfile
import tempfile

from reproducibility import print_artifact_info

image_hash_re = re.compile("((Successfully built )|(.*writing image sha256:))([0-9a-f]+).*")


def docker_build(args, dockerfile):
    """
    Runs 'docker build' and return image hash.

    Runs "docker build" with the given additional arguments to the call.
    The build logs will be passed through to stderr. Upon successful
    completion, the hash of the built docker image is returned by this
    function.
    """
    docker_args = ["docker", "build"] + args
    if dockerfile:
        docker_args.append("--file")
        docker_args.append(dockerfile)

    image_hash = None

    with subprocess.Popen(docker_args, stdout=subprocess.PIPE, stderr=subprocess.PIPE) as proc:
        # Pass the output generated by docker through so user potential
        # errors become visible in build logs.
        while True:
            line = proc.stdout.readline()
            if not line:
                break
            sys.stderr.buffer.write(line)
            sys.stderr.flush()

            if len(line) == 65:
                line_dec = line.decode("utf-8").strip()
                if re.match("[0-9a-f]", line_dec):
                    image_hash = line_dec

            # Try to parse the "completion" from the line received so
            # we can obtain the hash of the image.
            m = image_hash_re.match(line.decode("utf-8"))
            if m:
                image_hash = m.groups()[3]

        proc.wait()
        if proc.returncode != 0:
            for line in proc.stderr:
                print(line.decode("utf-8"), end="")
            raise RuntimeError("Docker build failed")

    if not image_hash:
        raise RuntimeError("Failed to obtain hash of built docker image")

    return image_hash


def _read_tar_contents(filename):
    """Reads tar file as map from filename -> content."""
    with tarfile.open(name=filename, mode="r|*") as tf:

        filemap = {}
        for member in tf:
            buf = member.tobuf()  # noqa - no idea why buf is here
            if member.type == tarfile.REGTYPE:
                filemap[member.name] = tf.extractfile(member).read()
            elif (member.type == tarfile.LNKTYPE) or (member.type == tarfile.SYMTYPE):
                filemap[member.name] = member.linkname[3:]
    return filemap


def _get_layer_data(filemap):
    """Gets the docker layer data from the filemap in correct order."""
    manifest = json.loads(filemap["manifest.json"])
    layers = manifest[0]["Layers"]

    out = []
    for layer in layers:
        if isinstance(filemap[layer], str):
            out.append(filemap[filemap[layer]])
        else:
            out.append(filemap[layer])

    return tuple(out)


class Inode(object):
    def __init__(self, mode, uid, gid, uid_name, gid_name):
        self.mode = mode
        self.uid = uid
        self.gid = gid
        self.uid_name = uid_name
        self.gid_name = gid_name


class DirInode(Inode):
    def __init__(self, mode, uid, gid, uid_name, gid_name, entries={}):
        Inode.__init__(self, mode, uid, gid, uid_name, gid_name)
        self.entries = entries.copy()


class LinkInode(Inode):
    def __init__(self, mode, uid, gid, uid_name, gid_name, target):
        Inode.__init__(self, mode, uid, gid, uid_name, gid_name)
        self.target = target


class RegInode(Inode):
    def __init__(self, mode, uid, gid, uid_name, gid_name, contents):
        Inode.__init__(self, mode, uid, gid, uid_name, gid_name)
        self.contents = contents


class FS:
    def __init__(self):
        self.root = DirInode(0o755, 0, 0, "root", "root")

    def clear_dir(self, path):
        self._lookup(path).entries.clear()

    def unlink(self, path):
        basename = os.path.basename(path)
        dirname = os.path.dirname(path)

        parent = self._lookup(dirname)
        del parent.entries[basename]

    def add_dir(self, path, mode, uid, gid, uid_name, gid_name):
        basename = os.path.basename(path)
        dirname = os.path.dirname(path)

        parent = self._lookup(dirname)
        if basename in parent.entries:
            if type(parent.entries[basename]) is not DirInode:
                raise RuntimeError("Expected entry is not a directory in base layer: " + path)
            else:
                parent.entries[basename].mode = mode
                parent.entries[basename].uid = uid
                parent.entries[basename].gid = gid
                parent.entries[basename].uid_name = uid_name
                parent.entries[basename].gid_name = gid_name
        else:
            parent.entries[basename] = DirInode(mode, uid, gid, uid_name, gid_name)

    def add_link(self, path, mode, uid, gid, uid_name, gid_name, target):
        basename = os.path.basename(path)
        dirname = os.path.dirname(path)

        self._lookup(dirname).entries[basename] = LinkInode(mode, uid, gid, uid_name, gid_name, target)

    def add_reg(self, path, mode, uid, gid, uid_name, gid_name, content):
        basename = os.path.basename(path)
        dirname = os.path.dirname(path)

        self._lookup(dirname).entries[basename] = RegInode(mode, uid, gid, uid_name, gid_name, content)

    def chmod(self, path, mode):
        inode = self._lookup(path)
        inode.mode = mode

    def _lookup(self, path):
        current = self.root
        for part in path.split("/"):
            if part != "":
                current = current.entries[part]
        return current

    def ls_dir(self, dir, indent=""):
        for key in sorted(dir.entries.keys()):
            print(indent + key + " " + str(dir.entries[key]))
            if type(dir.entries[key]) is DirInode:
                self.ls_dir(dir.entries[key], indent + " ")

    def ls(self):
        self.ls_dir(self.root)

    def write_hashes(self, output, dir=None, prefix="/"):
        if dir is None:
            dir = self.root
        for key in sorted(dir.entries.keys()):
            output.write(prefix)
            entry = dir.entries[key]
            if type(entry) is DirInode:
                output.write(f"{key}/\n")
                self.write_hashes(output, entry, f"  {prefix}{key}/")
            elif type(entry) is LinkInode:
                output.write(f"{key} -> {entry.target}\n")
            elif type(entry) is RegInode:
                hash = hashlib.sha256(entry.contents).hexdigest()
                output.write(f"{key} sha256#{hash}\n")


def _process_layer(layer, fs):
    tf = tarfile.open(fileobj=io.BytesIO(layer), mode="r")

    # Process all members in the tarfile. They are either ordinary
    # dirs/files/symlinks to be extracted, or they are "white-out" files:
    # These direct to delete certain underlying files from previous layer.
    for member in tf:
        basename = os.path.basename(member.path)
        dirname = os.path.dirname(member.path)
        if basename == ".wh..wh..opq":
            fs.clear_dir(dirname)
        elif basename.startswith(".wh."):
            fs.unlink(os.path.join(dirname, basename[4:]))
        else:
            if member.type == tarfile.DIRTYPE:
                fs.add_dir(member.path, member.mode, member.uid, member.gid, member.uname, member.gname)
            elif member.type == tarfile.REGTYPE or member.type == tarfile.AREGTYPE:
                fs.add_reg(
                    member.path,
                    member.mode,
                    member.uid,
                    member.gid,
                    member.uname,
                    member.gname,
                    tf.extractfile(member).read(),
                )
            elif member.type == tarfile.LNKTYPE or member.type == tarfile.SYMTYPE:
                fs.add_link(
                    member.path, member.mode, member.uid, member.gid, member.uname, member.gname, member.linkname
                )
            else:
                raise RuntimeError("Unhandled tar member kind")


def docker_extract_fs(image_hash):
    """
    Extracts the image via 'docker save' and builds fs.

    Extract the docker image identified by the given hash.
    Flatten all the layers and build a temporary in-memory
    filesystem representation of the image.
    """
    tar_name = tempfile.mktemp(suffix=".tar")
    atexit.register(lambda: os.remove(tar_name))
    with subprocess.Popen(["docker", "save", image_hash, "-o", tar_name], stdout=subprocess.PIPE) as proc:
        proc.wait()
        if proc.returncode != 0:
            raise RuntimeError("Docker save failed")
        layer_filemap = _read_tar_contents(tar_name)

    layers = _get_layer_data(layer_filemap)
    fs = FS()
    for layer in layers:
        _process_layer(layer, fs)

    # These files are not properly controlled inside docker: they are
    # bind-mounted, and docker build cannot set their permissions. Fix this
    # up on extracting the docker save file here.
    fs.chmod("etc/hosts", 0o644)
    fs.chmod("etc/hostname", 0o644)
    fs.chmod("etc/resolv.conf", 0o644)

    return fs


def _recurse_add_to_tar(path_prefix, dir_node, tf):
    for name in sorted(dir_node.entries.keys()):
        inode = dir_node.entries[name]
        ti = tarfile.TarInfo(path_prefix + name)
        ti.size = 0
        ti.mtime = 0
        ti.mode = inode.mode
        ti.uid = inode.uid
        ti.gid = inode.gid
        ti.uname = inode.uid_name
        ti.gname = inode.gid_name
        if type(inode) is DirInode:
            ti.type = tarfile.DIRTYPE
            tf.addfile(ti)
            _recurse_add_to_tar(path_prefix + name + "/", inode, tf)
        elif type(inode) is LinkInode:
            ti.type = tarfile.SYMTYPE
            ti.linkname = inode.target
            tf.addfile(ti)
        elif type(inode) is RegInode:
            ti.type = tarfile.AREGTYPE
            ti.size = len(inode.contents)
            tf.addfile(ti, io.BytesIO(inode.contents))
        else:
            raise RuntimeError("Unhandled inode kind")


def tar_fs(fs, outfile):
    """
    Tar up the filesystem tree.

    Recursively archive the given filesystem tree as a tar
    archive into the given file. All files are written with
    "zero" timestamps and in deterministic order in order
    to generate reproducible results.
    """
    tf = tarfile.open(fileobj=outfile, mode="w")

    _recurse_add_to_tar("", fs.root, tf)


def make_argparser():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-s", "--skip-pull", help="Don't attempt to pull image from dockerhub.", default=False, action="store_true"
    )
    parser.add_argument("-o", "--output", help="Target (tar) file to write to", type=str)
    parser.add_argument(
        "-d",
        "--dockerfile",
        type=str,
        default="",
        help="Name of the Dockerfile to target.",
    )
    parser.add_argument(
        "--extra-dockerfile",
        type=str,
        help="An additional dockerfile to be layered on top of the first image.",
    )
    parser.add_argument("--dev-root-ca", type=str, default="", help="Root CA for the dev image")
    parser.add_argument(
        "build_args",
        metavar="build_args",
        type=str,
        nargs="*",
        help="Extra args to pass to docker build",
    )
    return parser


def diff_hash_lists(out_file, build_args, fs):
    """
    If the expected.hash-list file exists, this function will output a list of
    hashes for the in-memory filesystem. It is intended to be used to figure out
    why a CI build produces a different tar to a local build:

    # Create an empty file to enable this functionality:
    touch ic-os/guestos/rootfs/expected.hash-list

    # Run Bazel to produce a hash listing for your local environment:
    bazel build //ic-os/guestos/dev:rootfs-tree.tar --disk_cache= --remote_cache=

    # Copy hash listing produced by Bazel to expected.hash-list:
    cp bazel-bin/ic-os/guestos/dev/rootfs-tree.tar.hash-list ic-os/guestos/rootfs/expected.hash-list

    # Commit and push expected.hash-list, then check CI logs to see the diff:
    git add ic-os/guestos/rootfs/expected.hash-list && git commit -m "Debugging (NOT TO BE MERGED)"
    """
    actual_hash_list = f"{out_file}.hash-list"
    with open(actual_hash_list, "w") as file:
        if len(build_args) == 0:
            return

        expected_hash_list = os.path.join(build_args[0], "expected.hash-list")
        if not os.path.isfile(expected_hash_list):
            return

        fs.write_hashes(file)
        file.flush()

        diff_cmd = ["diff", expected_hash_list, actual_hash_list]
        print(f"running: {diff_cmd}", file=sys.stderr)
        subprocess.run(diff_cmd)


def main():
    args = make_argparser().parse_args(sys.argv[1:])

    out_file = args.output
    dockerfile = args.dockerfile
    extra_dockerfile = args.extra_dockerfile
    build_args = list(args.build_args)
    extra_args = list(args.build_args)

    # Build the docker image.
    if not args.skip_pull:
        build_args.append("--pull")
    if any(
        [
            os.environ.get("CI_JOB_NAME", "").startswith("docker-build-ic"),
            os.environ.get("CI_COMMIT_REF_PROTECTED", "false") == "true",
        ]
    ):
        build_args.append("--no-cache")
        extra_args.append("--no-cache")
    image_hash = docker_build(build_args, dockerfile)

    # If an additional Dockerfile is specified, build an additional layer on top of the one already built
    if extra_dockerfile:
        extra_args.append("--build-arg")
        extra_args.append("PREVIOUS_IMAGE=%s" % image_hash)
        if args.dev_root_ca != "":
            try:
                ca_contents = open(args.dev_root_ca, "r").read()
                extra_args.append("--build-arg")
                extra_args.append("DEV_ROOT_CA=" + ca_contents)
            except FileNotFoundError:
                print(f"WARNING: Skipping --dev-ca-root file {args.dev_root_ca}, which does not exist")
        image_hash = docker_build(extra_args, extra_dockerfile)

    # Extract and flatten all layers, build an in-memory pseudo filesystem
    # representing the docker image.
    fs = docker_extract_fs(image_hash)

    # Export the filesystem tree as a tar file.
    tar_fs(fs, open(out_file, "wb"))

    # Diff filesystem against an expected hash list if one is provided.
    diff_hash_lists(out_file, build_args, fs)

    print_artifact_info(out_file)


if __name__ == "__main__":
    main()
